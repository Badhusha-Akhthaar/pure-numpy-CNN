{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('001.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process data and calculate indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 2559 characters, 65 unique\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_size, X_size = len(data), len(chars)\n",
    "print(\"data has %d characters, %d unique\" % (data_size, X_size))\n",
    "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
    "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_size = 100 # Size of the hidden layer\n",
    "T_steps = 25 # Number of time steps (length of the sequence) used for training\n",
    "learning_rate = 1e-1 # Learning rate\n",
    "weight_sd = 0.1 # Standard deviation of weights for initialization\n",
    "z_size = H_size + X_size # Size of concatenate(H, X) vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions and Derivatives\n",
    "\n",
    "#### Sigmoid\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma(x) &= \\frac{1}{1 + e^{-x}}\\\\\n",
    "\\frac{d\\sigma(x)}{dx} &= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "\n",
    "#### Tanh\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d\\text{tanh}(x)}{dx} &= 1 - \\text{tanh}^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def dtanh(y):\n",
    "    return 1 - y * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Param:\n",
    "    def __init__(self, name, value):\n",
    "        self.name = name\n",
    "        self.v = value #parameter value\n",
    "        self.d = np.zeros_like(value) #derivative\n",
    "        self.m = np.zeros_like(value) #momentum for AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random weights with normal distribution (`0`, `weight_sd`) for $tanh$ activation function and (`0.5`, `weight_sd`) for $sigmoid$ activation function.\n",
    "\n",
    "Biases are initialized to zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameters:\n",
    "    def __init__(self):\n",
    "        self.W_f = Param('W_f', \n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_f = Param('b_f',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_i = Param('W_i',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_i = Param('b_i',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_C = Param('W_C',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd)\n",
    "        self.b_C = Param('b_C',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        self.W_o = Param('W_o',\n",
    "                         np.random.randn(H_size, z_size) * weight_sd + 0.5)\n",
    "        self.b_o = Param('b_o',\n",
    "                         np.zeros((H_size, 1)))\n",
    "\n",
    "        #For final layer to predict the next character\n",
    "        self.W_v = Param('W_v',\n",
    "                         np.random.randn(X_size, H_size) * weight_sd)\n",
    "        self.b_v = Param('b_v',\n",
    "                         np.zeros((X_size, 1)))\n",
    "        \n",
    "    def all(self):\n",
    "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
    "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
    "        \n",
    "parameters = Parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "![LSTM](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
    "\n",
    "*Operation $z$ is the concatenation of $x$ and $h_{t-1}$*\n",
    "\n",
    "#### Concatenation of $h_{t-1}$ and $x_t$\n",
    "\\begin{align}\n",
    "z & = [h_{t-1}, x_t] \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### LSTM functions\n",
    "\\begin{align}\n",
    "f_t & = \\sigma(W_f \\cdot z + b_f) \\\\\n",
    "i_t & = \\sigma(W_i \\cdot z + b_i) \\\\\n",
    "\\bar{C}_t & = tanh(W_C \\cdot z + b_C) \\\\\n",
    "C_t & = f_t * C_{t-1} + i_t * \\bar{C}_t \\\\\n",
    "o_t & = \\sigma(W_o \\cdot z + b_t) \\\\\n",
    "h_t &= o_t * tanh(C_t) \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Logits\n",
    "\\begin{align}\n",
    "v_t &= W_v \\cdot h_t + b_v \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Softmax\n",
    "\\begin{align}\n",
    "\\hat{y_t} &= \\text{softmax}(v_t)\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_t}$ is `y` in code and $y_t$ is `targets`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, h_prev, C_prev, p = parameters):\n",
    "    assert x.shape == (X_size, 1)\n",
    "    assert h_prev.shape == (H_size, 1)\n",
    "    assert C_prev.shape == (H_size, 1)\n",
    "    \n",
    "    z = np.row_stack((h_prev, x))\n",
    "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
    "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
    "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
    "\n",
    "    C = f * C_prev + i * C_bar\n",
    "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
    "    h = o * tanh(C)\n",
    "\n",
    "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
    "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
    "\n",
    "    return z, f, i, C_bar, C, o, h, v, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "#### Loss\n",
    "\n",
    "\\begin{align}\n",
    "L_k &= -\\sum_{t=k}^T\\sum_j y_{t,j} log \\hat{y_{t,j}} \\\\\n",
    "L &= L_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "#### Gradients\n",
    "\n",
    "\\begin{align}\n",
    "dv_t &= \\hat{y_t} - y_t \\\\\n",
    "dh_t &= dh'_t + W_y^T \\cdot dv_t \\\\\n",
    "do_t &= dh_t * \\text{tanh}(C_t) \\\\\n",
    "dC_t &= dC'_t + dh_t * o_t * (1 - \\text{tanh}^2(C_t))\\\\\n",
    "d\\bar{C}_t &= dC_t * i_t \\\\\n",
    "di_t &= dC_t * \\bar{C}_t \\\\\n",
    "df_t &= dC_t * C_{t-1} \\\\\n",
    "\\\\\n",
    "df'_t &= f_t * (1 - f_t) * df_t \\\\\n",
    "di'_t &= i_t * (1 - i_t) * di_t \\\\\n",
    "d\\bar{C}'_{t-1} &= (1 - \\bar{C}_t^2) * d\\bar{C}_t \\\\\n",
    "do'_t &= o_t * (1 - o_t) * do_t \\\\\n",
    "dz_t &= W_f^T \\cdot df'_t \\\\\n",
    "     &+ W_i^T \\cdot di_t \\\\\n",
    "     &+ W_C^T \\cdot d\\bar{C}_t \\\\\n",
    "     &+ W_o^T \\cdot do_t \\\\\n",
    "\\\\\n",
    "[dh'_{t-1}, dx_t] &= dz_t \\\\\n",
    "dC'_t &= f_t * dC_t\n",
    "\\end{align}\n",
    "\n",
    "* $dC'_t = \\frac{\\partial L_{t+1}}{\\partial C_t}$ and $dh'_t = \\frac{\\partial L_{t+1}}{\\partial h_t}$\n",
    "* $dC_t = \\frac{\\partial L}{\\partial C_t} = \\frac{\\partial L_t}{\\partial C_t}$ and $dh_t = \\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_{t}}{\\partial h_t}$\n",
    "* All other derivatives are of $L$\n",
    "* `target` is target character index $y_t$\n",
    "* `dh_next` is $dh'_{t}$ (size H x 1)\n",
    "* `dC_next` is $dC'_{t}$ (size H x 1)\n",
    "* `C_prev` is $C_{t-1}$ (size H x 1)\n",
    "* $df'_t$, $di'_t$, $d\\bar{C}'_t$, and $do'_t$ are *also* assigned to `df`, `di`, `dC_bar`, and `do` in the **code**.\n",
    "* *Returns* $dh_t$ and $dC_t$\n",
    "\n",
    "#### Model parameter gradients\n",
    "\n",
    "\\begin{align}\n",
    "dW_v &= dv_t \\cdot h_t^T \\\\\n",
    "db_v &= dv_t \\\\\n",
    "\\\\\n",
    "dW_f &= df'_t \\cdot z^T \\\\\n",
    "db_f &= df'_t \\\\\n",
    "\\\\\n",
    "dW_i &= di'_t \\cdot z^T \\\\\n",
    "db_i &= di'_t \\\\\n",
    "\\\\\n",
    "dW_C &= d\\bar{C}'_t \\cdot z^T \\\\\n",
    "db_C &= d\\bar{C}'_t \\\\\n",
    "\\\\\n",
    "dW_o &= do'_t \\cdot z^T \\\\\n",
    "db_o &= do'_t \\\\\n",
    "\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(target, dh_next, dC_next, C_prev,\n",
    "             z, f, i, C_bar, C, o, h, v, y,\n",
    "             p = parameters):\n",
    "    \n",
    "    assert z.shape == (X_size + H_size, 1)\n",
    "    assert v.shape == (X_size, 1)\n",
    "    assert y.shape == (X_size, 1)\n",
    "    \n",
    "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
    "        assert param.shape == (H_size, 1)\n",
    "        \n",
    "    dv = np.copy(y)\n",
    "    dv[target] -= 1\n",
    "\n",
    "    p.W_v.d += np.dot(dv, h.T)\n",
    "    p.b_v.d += dv\n",
    "\n",
    "    dh = np.dot(p.W_v.v.T, dv)        \n",
    "    dh += dh_next\n",
    "    do = dh * tanh(C)\n",
    "    do = dsigmoid(o) * do\n",
    "    p.W_o.d += np.dot(do, z.T)\n",
    "    p.b_o.d += do\n",
    "\n",
    "    dC = np.copy(dC_next)\n",
    "    dC += dh * o * dtanh(tanh(C))\n",
    "    dC_bar = dC * i\n",
    "    dC_bar = dtanh(C_bar) * dC_bar\n",
    "    p.W_C.d += np.dot(dC_bar, z.T)\n",
    "    p.b_C.d += dC_bar\n",
    "\n",
    "    di = dC * C_bar\n",
    "    di = dsigmoid(i) * di\n",
    "    p.W_i.d += np.dot(di, z.T)\n",
    "    p.b_i.d += di\n",
    "\n",
    "    df = dC * C_prev\n",
    "    df = dsigmoid(f) * df\n",
    "    p.W_f.d += np.dot(df, z.T)\n",
    "    p.b_f.d += df\n",
    "\n",
    "    dz = (np.dot(p.W_f.v.T, df)\n",
    "         + np.dot(p.W_i.v.T, di)\n",
    "         + np.dot(p.W_C.v.T, dC_bar)\n",
    "         + np.dot(p.W_o.v.T, do))\n",
    "    dh_prev = dz[:H_size, :]\n",
    "    dC_prev = f * dC\n",
    "    \n",
    "    return dh_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear gradients before each backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.d.fill(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clip gradients to mitigate exploding gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(params = parameters):\n",
    "    for p in params.all():\n",
    "        np.clip(p.d, -1, 1, out=p.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
    "\n",
    "* `input`, `target` are list of integers, with character indexes.\n",
    "* `h_prev` is the array of initial `h` at $h_{-1}$ (size H x 1)\n",
    "* `C_prev` is the array of initial `C` at $C_{-1}$ (size H x 1)\n",
    "* *Returns* loss, final $h_T$ and $C_T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward(inputs, targets, h_prev, C_prev):\n",
    "    global paramters\n",
    "    \n",
    "    # To store the values for each time step\n",
    "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
    "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
    "    v_s, y_s =  {}, {}\n",
    "    \n",
    "    # Values at t - 1\n",
    "    h_s[-1] = np.copy(h_prev)\n",
    "    C_s[-1] = np.copy(C_prev)\n",
    "    \n",
    "    loss = 0\n",
    "    # Loop through time steps\n",
    "    assert len(inputs) == T_steps\n",
    "    for t in range(len(inputs)):\n",
    "        x_s[t] = np.zeros((X_size, 1))\n",
    "        x_s[t][inputs[t]] = 1 # Input character\n",
    "        \n",
    "        (z_s[t], f_s[t], i_s[t],\n",
    "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
    "        v_s[t], y_s[t]) = \\\n",
    "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
    "            \n",
    "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
    "        \n",
    "    clear_gradients()\n",
    "\n",
    "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
    "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
    "\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        # Backward pass\n",
    "        dh_next, dC_next = \\\n",
    "            backward(target = targets[t], dh_next = dh_next,\n",
    "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
    "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
    "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
    "                     y = y_s[t])\n",
    "\n",
    "    clip_gradients()\n",
    "        \n",
    "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the next character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
    "    x = np.zeros((X_size, 1))\n",
    "    x[first_char_idx] = 1\n",
    "\n",
    "    h = h_prev\n",
    "    C = C_prev\n",
    "\n",
    "    indexes = []\n",
    "    \n",
    "    for t in range(sentence_length):\n",
    "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
    "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
    "        x = np.zeros((X_size, 1))\n",
    "        x[idx] = 1\n",
    "        indexes.append(idx)\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training (Adagrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the graph and display a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_status(inputs, h_prev, C_prev):\n",
    "    #initialized later\n",
    "    global plot_iter, plot_loss\n",
    "    global smooth_loss\n",
    "    \n",
    "    # Get predictions for 200 letters with current model\n",
    "\n",
    "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
    "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
    "\n",
    "    # Clear and plot\n",
    "    plt.plot(plot_iter, plot_loss)\n",
    "    display.clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "    #Print prediction and loss\n",
    "    print(\"----\\n %s \\n----\" % (txt, ))\n",
    "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update parameters\n",
    "\n",
    "\\begin{align}\n",
    "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
    "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_paramters(params = parameters):\n",
    "    for p in params.all():\n",
    "        p.m += p.d * p.d # Calculate sum of gradients\n",
    "        #print(learning_rate * dparam)\n",
    "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To delay the keyboard interrupt to prevent the training \n",
    "from stopping in the middle of an iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "\n",
    "class DelayedKeyboardInterrupt(object):\n",
    "    def __enter__(self):\n",
    "        self.signal_received = False\n",
    "        self.old_handler = signal.signal(signal.SIGINT, self.handler)\n",
    "\n",
    "    def handler(self, sig, frame):\n",
    "        self.signal_received = (sig, frame)\n",
    "        print('SIGINT received. Delaying KeyboardInterrupt.')\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.signal(signal.SIGINT, self.old_handler)\n",
    "        if self.signal_received:\n",
    "            self.old_handler(*self.signal_received)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential average of loss\n",
    "# Initialize to a error of a random model\n",
    "smooth_loss = -np.log(1.0 / X_size) * T_steps\n",
    "\n",
    "iteration, pointer = 0, 0\n",
    "\n",
    "# For the graph\n",
    "plot_iter = np.zeros((0))\n",
    "plot_loss = np.zeros((0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " rt reserves, which it was unable to siac and int on choofits a suanc ortoms fourt is clrsit is cliss 's prom -oice bigle, AOL Europe an alvision was pooflomadvilisto\"y sale from $639m year-earlier.\n",
      "\n",
      "T \n",
      "----\n",
      "iter 29200, loss 0.324305\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        with DelayedKeyboardInterrupt():\n",
    "            # Reset\n",
    "            if pointer + T_steps >= len(data) or iteration == 0:\n",
    "                g_h_prev = np.zeros((H_size, 1))\n",
    "                g_C_prev = np.zeros((H_size, 1))\n",
    "                pointer = 0\n",
    "\n",
    "\n",
    "            inputs = ([char_to_idx[ch] \n",
    "                       for ch in data[pointer: pointer + T_steps]])\n",
    "            targets = ([char_to_idx[ch] \n",
    "                        for ch in data[pointer + 1: pointer + T_steps + 1]])\n",
    "\n",
    "            loss, g_h_prev, g_C_prev = \\\n",
    "                forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "            # Print every hundred steps\n",
    "            if iteration % 100 == 0:\n",
    "                update_status(inputs, g_h_prev, g_C_prev)\n",
    "\n",
    "            update_paramters()\n",
    "\n",
    "            plot_iter = np.append(plot_iter, [iteration])\n",
    "            plot_loss = np.append(plot_loss, [loss])\n",
    "\n",
    "            pointer += T_steps\n",
    "            iteration += 1\n",
    "    except KeyboardInterrupt:\n",
    "        update_status(inputs, g_h_prev, g_C_prev)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Check\n",
    "\n",
    "Approximate the numerical gradients by changing parameters and running the model. Check if the approximated gradients are equal to the computed analytical gradients (by backpropagation).\n",
    "\n",
    "Try this on `num_checks` individual paramters picked randomly for each weight matrix and bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_numerical_gradient(param, idx, delta, inputs, target, h_prev, C_prev):\n",
    "    old_val = param.v.flat[idx]\n",
    "    \n",
    "    # evaluate loss at [x + delta] and [x - delta]\n",
    "    param.v.flat[idx] = old_val + delta\n",
    "    loss_plus_delta, _, _ = forward_backward(inputs, targets,\n",
    "                                             h_prev, C_prev)\n",
    "    param.v.flat[idx] = old_val - delta\n",
    "    loss_mins_delta, _, _ = forward_backward(inputs, targets, \n",
    "                                             h_prev, C_prev)\n",
    "    \n",
    "    param.v.flat[idx] = old_val #reset\n",
    "\n",
    "    grad_numerical = (loss_plus_delta - loss_mins_delta) / (2 * delta)\n",
    "    # Clip numerical error because analytical gradient is clipped\n",
    "    [grad_numerical] = np.clip([grad_numerical], -1, 1) \n",
    "    \n",
    "    return grad_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check gradient of each paramter matrix/vector at `num_checks` individual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(num_checks, delta, inputs, target, h_prev, C_prev):\n",
    "    global parameters\n",
    "    \n",
    "    # To calculate computed gradients\n",
    "    _, _, _ =  forward_backward(inputs, targets, h_prev, C_prev)\n",
    "    \n",
    "    \n",
    "    for param in parameters.all():\n",
    "        #Make a copy because this will get modified\n",
    "        d_copy = np.copy(param.d)\n",
    "\n",
    "        # Test num_checks times\n",
    "        for i in range(num_checks):\n",
    "            # Pick a random index\n",
    "            rnd_idx = int(uniform(0, param.v.size))\n",
    "            \n",
    "            grad_numerical = calc_numerical_gradient(param,\n",
    "                                                     rnd_idx,\n",
    "                                                     delta,\n",
    "                                                     inputs,\n",
    "                                                     target,\n",
    "                                                     h_prev, C_prev)\n",
    "            grad_analytical = d_copy.flat[rnd_idx]\n",
    "\n",
    "            err_sum = abs(grad_numerical + grad_analytical) + 1e-09\n",
    "            rel_error = abs(grad_analytical - grad_numerical) / err_sum\n",
    "            \n",
    "            # If relative error is greater than 1e-06\n",
    "            if rel_error > 1e-06:\n",
    "                print('%s (%e, %e) => %e'\n",
    "                      % (param.name, grad_numerical, grad_analytical, rel_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gradient_check(10, 1e-5, inputs, targets, g_h_prev, g_C_prev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
